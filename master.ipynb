{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTwV0adBgn6H8WRluQxcFP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chaitanya-Aggarwal/widsSubmission/blob/master/master.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7O_PoIHpvUfc",
        "outputId": "f3e8143a-f5b2-40b6-fde2-f9450b8fb38a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive/MyDrive/\n",
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGmCP_8Fvn-r",
        "outputId": "58868935-9f63-4c27-f84e-8261261677c5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n",
            "\u001b[0m\u001b[01;34m'Colab Notebooks'\u001b[0m/   \u001b[01;34mmodels\u001b[0m/        test_model.py     vgg.py\n",
            " \u001b[01;34mebb_dataset\u001b[0m/        \u001b[01;34m__pycache__\u001b[0m/   train_model.py    \u001b[01;34mvisual_samples\u001b[0m/\n",
            " load_dataset.py     random.py      Untitled0.ipynb\n",
            " main.py             random.txt     utils.py\n",
            " master.ipynb        \u001b[01;34mresults\u001b[0m/       \u001b[01;34mvgg_pretrained\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2020 by Andrey Ignatov. All Rights Reserved.\n",
        "\n",
        "# *****************************LOAD DATASET*************************************************************\n",
        "\n",
        "from __future__ import print_function\n",
        "from scipy import misc\n",
        "from PIL import Image\n",
        "import imageio\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "import scipy.io\n",
        "import imageio\n",
        "#*************************************VGG****************************************************\n",
        "IMAGE_MEAN = np.array([123.68,  116.779,  103.939])\n",
        "\n",
        "\n",
        "def _conv_layer_(input, weights, bias):\n",
        "    print(\"no errors here\")\n",
        "    conv = tf.nn.conv2d(input, tf.constant(weights), strides=(1, 1, 1, 1), padding='SAME')\n",
        "    return tf.nn.bias_add(conv, bias)\n",
        "\n",
        "def net(path_to_vgg_net, input_image):\n",
        "\n",
        "    layers = (\n",
        "        'conv1_1', 'relu1_1', 'conv1_2', 'relu1_2', 'pool1',\n",
        "\n",
        "        'conv2_1', 'relu2_1', 'conv2_2', 'relu2_2', 'pool2',\n",
        "\n",
        "        'conv3_1', 'relu3_1', 'conv3_2', 'relu3_2', 'conv3_3',\n",
        "        'relu3_3', 'conv3_4', 'relu3_4', 'pool3',\n",
        "\n",
        "        'conv4_1', 'relu4_1', 'conv4_2', 'relu4_2', 'conv4_3',\n",
        "        'relu4_3', 'conv4_4', 'relu4_4', 'pool4',\n",
        "\n",
        "        'conv5_1', 'relu5_1', 'conv5_2', 'relu5_2', 'conv5_3',\n",
        "        'relu5_3', 'conv5_4', 'relu5_4'\n",
        "    )\n",
        "\n",
        "    data = scipy.io.loadmat(path_to_vgg_net)\n",
        "    weights = data['layers'][0]\n",
        "\n",
        "    net = {}\n",
        "    current = input_image\n",
        "    for i, name in enumerate(layers):\n",
        "        layer_type = name[:4]\n",
        "        if layer_type == 'conv':\n",
        "            kernels, bias = weights[i][0][0][0][0]\n",
        "            kernels = np.transpose(kernels, (1, 0, 2, 3))\n",
        "            bias = bias.reshape(-1)\n",
        "            current = _conv_layer_(current, kernels, bias)\n",
        "        elif layer_type == 'relu':\n",
        "            current = tf.nn.relu(current)\n",
        "        elif layer_type == 'pool':\n",
        "            current = _pool_layer(current)\n",
        "        net[name] = current\n",
        "\n",
        "    return net\n",
        "\n",
        "\n",
        "\n",
        "def _pool_layer(input):\n",
        "    return tf.nn.max_pool(input, ksize=(1, 2, 2, 1), strides=(1, 2, 2, 1), padding='SAME')\n",
        "\n",
        "\n",
        "def preprocess(image):\n",
        "    return image - IMAGE_MEAN\n",
        "\n",
        "#*************************************UTILS*************************************************\n",
        "\n",
        "# Copyright 2020 by Andrey Ignatov. All Rights Reserved.\n",
        "\n",
        "from functools import reduce\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "\n",
        "NUM_DEFAULT_TRAIN_ITERS = [-1, 100000, 80000, 30000, 20000, 20000, 5000, 5000]\n",
        "\n",
        "\n",
        "def process_command_args(arguments):\n",
        "\n",
        "    # Specifying the default parameters\n",
        "\n",
        "    level = 1\n",
        "    batch_size = 50\n",
        "\n",
        "    train_size = 4894\n",
        "    learning_rate = 5e-5\n",
        "\n",
        "    eval_step = 1000\n",
        "    restore_iter = None\n",
        "    num_train_iters = None\n",
        "\n",
        "    dataset_dir = 'ebb_dataset/'\n",
        "    vgg_dir = 'vgg_pretrained/imagenet-vgg-verydeep-19.mat'\n",
        "\n",
        "    for args in arguments:\n",
        "\n",
        "        if args.startswith(\"level\"):\n",
        "            level = int(args.split(\"=\")[1])\n",
        "\n",
        "        if args.startswith(\"batch_size\"):\n",
        "            batch_size = int(args.split(\"=\")[1])\n",
        "\n",
        "        if args.startswith(\"train_size\"):\n",
        "            train_size = int(args.split(\"=\")[1])\n",
        "\n",
        "        if args.startswith(\"learning_rate\"):\n",
        "            learning_rate = float(args.split(\"=\")[1])\n",
        "\n",
        "        if args.startswith(\"restore_iter\"):\n",
        "            restore_iter = int(args.split(\"=\")[1])\n",
        "\n",
        "        if args.startswith(\"num_train_iters\"):\n",
        "            num_train_iters = int(args.split(\"=\")[1])\n",
        "\n",
        "        # -----------------------------------\n",
        "\n",
        "        if args.startswith(\"dataset_dir\"):\n",
        "            dataset_dir = args.split(\"=\")[1]\n",
        "\n",
        "        if args.startswith(\"vgg_dir\"):\n",
        "            vgg_dir = args.split(\"=\")[1]\n",
        "\n",
        "        if args.startswith(\"eval_step\"):\n",
        "            eval_step = int(args.split(\"=\")[1])\n",
        "\n",
        "    if restore_iter is None and level < 7:\n",
        "        restore_iter = get_last_iter(level + 1)\n",
        "        if restore_iter == -1:\n",
        "            print(\"Error: Cannot find any pre-trained models for PyNET's level \" + str(level + 1) + \".\")\n",
        "            print(\"Aborting the training.\")\n",
        "            sys.exit()\n",
        "\n",
        "    if num_train_iters is None:\n",
        "        num_train_iters = NUM_DEFAULT_TRAIN_ITERS[level]\n",
        "\n",
        "    print(\"The following parameters will be applied for CNN training:\")\n",
        "\n",
        "    print(\"Training level: \" + str(level))\n",
        "    print(\"Batch size: \" + str(batch_size))\n",
        "    print(\"Learning rate: \" + str(learning_rate))\n",
        "    print(\"Training iterations: \" + str(num_train_iters))\n",
        "    print(\"Evaluation step: \" + str(eval_step))\n",
        "    print(\"Restore Iteration: \" + str(restore_iter))\n",
        "    print(\"Path to the dataset: \" + dataset_dir)\n",
        "    print(\"Path to VGG-19 network: \" + vgg_dir)\n",
        "\n",
        "    return level, batch_size, train_size, learning_rate, restore_iter, num_train_iters,\\\n",
        "           dataset_dir, vgg_dir, eval_step\n",
        "\n",
        "\n",
        "def process_test_model_args(arguments):\n",
        "\n",
        "    level = 1\n",
        "    restore_iter = None\n",
        "\n",
        "    dataset_dir = 'ebb_dataset/'\n",
        "    use_gpu = \"true\"\n",
        "\n",
        "    orig_model = \"false\"\n",
        "\n",
        "    for args in arguments:\n",
        "\n",
        "        if args.startswith(\"level\"):\n",
        "            level = int(args.split(\"=\")[1])\n",
        "\n",
        "        if args.startswith(\"dataset_dir\"):\n",
        "            dataset_dir = args.split(\"=\")[1]\n",
        "\n",
        "        if args.startswith(\"restore_iter\"):\n",
        "            restore_iter = int(args.split(\"=\")[1])\n",
        "\n",
        "        if args.startswith(\"use_gpu\"):\n",
        "            use_gpu = args.split(\"=\")[1]\n",
        "\n",
        "        if args.startswith(\"orig\"):\n",
        "            orig_model = args.split(\"=\")[1]\n",
        "\n",
        "    if restore_iter is None and orig_model == \"false\":\n",
        "        restore_iter = get_last_iter(level)\n",
        "        if restore_iter == -1:\n",
        "            print(\"Error: Cannot find any pre-trained models for PyNET's level \" + str(level) + \".\")\n",
        "            sys.exit()\n",
        "\n",
        "    return level, restore_iter, dataset_dir, use_gpu, orig_model\n",
        "\n",
        "\n",
        "def get_last_iter(level):\n",
        "\n",
        "    saved_models = [int((model_file.split(\"_\")[-1]).split(\".\")[0])\n",
        "                    for model_file in os.listdir(\"models/\")\n",
        "                    if model_file.startswith(\"pynet_level_\" + str(level))]\n",
        "\n",
        "    if len(saved_models) > 0:\n",
        "        return np.max(saved_models)\n",
        "    else:\n",
        "        return -1\n",
        "\n",
        "\n",
        "def log10(x):\n",
        "    numerator = tf.compat.v1.log(x)\n",
        "    denominator = tf.compat.v1.log(tf.constant(10, dtype=numerator.dtype))\n",
        "    return numerator / denominator\n",
        "\n",
        "\n",
        "def _tensor_size(tensor):\n",
        "    from operator import mul\n",
        "    return reduce(mul, (d.value for d in tensor.get_shape()[1:]), 1)\n",
        "\n",
        "#*************************************UTILS*************************************************\n",
        "\n",
        "\n",
        "\n",
        "#********************************LOAD DATASET************************************************************\n",
        "def load_test_data(dataset_dir, PATCH_WIDTH, PATCH_HEIGHT, DSLR_SCALE):\n",
        "\n",
        "    test_directory_orig = dataset_dir + 'test/original/'\n",
        "    test_directory_orig_depth = dataset_dir + 'test/original_depth/'\n",
        "    test_directory_blur = dataset_dir + 'test/bokeh/'\n",
        "\n",
        "    NUM_TEST_IMAGES = 48\n",
        "    # NUM_TEST_IMAGES = len([name for name in os.listdir(test_directory_orig)\n",
        "                          #  if os.path.isfile(os.path.join(test_directory_orig, name))])\n",
        "\n",
        "    test_data = np.zeros((NUM_TEST_IMAGES, PATCH_HEIGHT, PATCH_WIDTH, 4))\n",
        "    test_answ = np.zeros((NUM_TEST_IMAGES, int(PATCH_HEIGHT * DSLR_SCALE), int(PATCH_WIDTH * DSLR_SCALE), 3))\n",
        "\n",
        "    for i in range(0, NUM_TEST_IMAGES):\n",
        "        i += 2001\n",
        "        I = imageio.imread(test_directory_orig + str(i) + '.jpg')\n",
        "        I_depth = imageio.imread(test_directory_orig_depth + str(i) + '.jpg')\n",
        "\n",
        "        # Downscaling the image by a factor of 2\n",
        "        I = imageio.imresize(I, 0.5, interp='bicubic')\n",
        "\n",
        "        # Making sure that its width is multiple of 32\n",
        "        new_width = int(I.shape[1]/32) * 32\n",
        "        I = I[:, 0:new_width, :]\n",
        "\n",
        "        # Stacking the image together with its depth map\n",
        "        I_temp = np.zeros((I.shape[0], I.shape[1], 4))\n",
        "        I_temp[:, :, 0:3] = I\n",
        "        I_temp[:, :, 3] = I_depth\n",
        "        I = I_temp\n",
        "\n",
        "        h, w, d = I.shape\n",
        "        y = np.random.randint(0, w - 512)\n",
        "\n",
        "        # Extracting random patch of width PATCH_WIDTH\n",
        "        I = np.float32(I[:, y:y + PATCH_WIDTH, :]) / 255.0\n",
        "        test_data[i, :] = I\n",
        "\n",
        "        I = imageio.imread(test_directory_blur + str(i) + '.jpg')\n",
        "        I = np.float32(imageio.imresize(I[:, y*2:y*2 + 1024, :], DSLR_SCALE / 2, interp='bicubic')) / 255.0\n",
        "        test_answ[i, :] = I\n",
        "        i-=2001\n",
        "    return test_data, test_answ\n",
        "\n",
        "\n",
        "def load_training_batch(dataset_dir, PATCH_WIDTH, PATCH_HEIGHT, DSLR_SCALE, train_size):\n",
        "\n",
        "    test_directory_orig = dataset_dir + 'train/original/'\n",
        "    test_directory_orig_depth = dataset_dir + 'train/original_depth/'\n",
        "    test_directory_blur = dataset_dir + 'train/bokeh/'\n",
        "\n",
        "    NUM_TRAINING_IMAGES = 500\n",
        "    # NUM_TRAINING_IMAGES = len([name for name in os.listdir(test_directory_orig)\n",
        "                          #  if os.path.isfile(os.path.join(test_directory_orig, name))])\n",
        "\n",
        "    TRAIN_IMAGES = np.random.choice(np.arange(0, NUM_TRAINING_IMAGES), train_size, replace=False)\n",
        "\n",
        "    test_data = np.zeros((train_size, PATCH_HEIGHT, PATCH_WIDTH, 4))\n",
        "    test_answ = np.zeros((train_size, int(PATCH_HEIGHT * DSLR_SCALE), int(PATCH_WIDTH * DSLR_SCALE), 3))\n",
        "\n",
        "    i = 0\n",
        "    for img in TRAIN_IMAGES:\n",
        "\n",
        "        I = imageio.imread(test_directory_orig + str(img) + '.jpg')\n",
        "        I_depth = imageio.imread(test_directory_orig_depth + str(img) + '.png')\n",
        "\n",
        "        # Downscaling the image by a factor of 2\n",
        "        I = imageio.imresize(I, 0.5, interp='bicubic')\n",
        "\n",
        "        # Making sure that its width is multiple of 32\n",
        "        new_width = int(I.shape[1] / 32) * 32\n",
        "        I = I[:, 0:new_width, :]\n",
        "\n",
        "        # Stacking the image together with its depth map\n",
        "        I_temp = np.zeros((I.shape[0], I.shape[1], 4))\n",
        "        I_temp[:, :, 0:3] = I\n",
        "        I_temp[:, :, 3] = I_depth\n",
        "        I = I_temp\n",
        "\n",
        "        h, w, d = I.shape\n",
        "        y = np.random.randint(0, w - 512)\n",
        "\n",
        "        # Extracting random patch of width PATCH_WIDTH\n",
        "        I = np.float32(I[:, y:y + PATCH_WIDTH, :]) / 255.0\n",
        "        test_data[i, :] = I\n",
        "\n",
        "        I = imageio.imread(test_directory_blur + str(img) + '.jpg')\n",
        "        I = np.float32(imageio.imresize(I[:, y * 2:y * 2 + 1024, :], DSLR_SCALE / 2, interp='bicubic')) / 255.0\n",
        "        test_answ[i, :] = I\n",
        "\n",
        "        i += 1\n",
        "\n",
        "    return test_data, test_answ\n",
        "\n",
        "\n",
        "def load_input_image(image_dir, depth_maps_dir, photo):\n",
        "\n",
        "    I = imageio.imread(image_dir + photo)\n",
        "    I_depth = imageio.imread(depth_maps_dir + str(photo.split(\".\")[0]) + '.png')\n",
        "\n",
        "    # Downscaling the image by a factor of 2\n",
        "    I = imageio.imresize(I, 0.5, interp='bicubic')\n",
        "\n",
        "    # Making sure that its width is multiple of 32\n",
        "    new_width = int(I.shape[1] / 32) * 32\n",
        "    I = I[:, 0:new_width, :]\n",
        "    I_depth = I_depth[:, 0:new_width]\n",
        "\n",
        "    # Stacking the image together with its depth map\n",
        "    I_temp = np.zeros((I.shape[0], I.shape[1], 4))\n",
        "    I_temp[:, :, 0:3] = I\n",
        "    I_temp[:, :, 3] = I_depth\n",
        "\n",
        "    I = np.float32(I_temp) / 255.0\n",
        "    I = np.reshape(I, [1, I.shape[0], I.shape[1], 4])\n",
        "\n",
        "    return I\n",
        "#********************************LOAD DATASET************************************************************\n",
        "\n",
        "\n",
        "#********************************MODEL.py************************************************************\n",
        "def PyNET(input, instance_norm=True, instance_norm_level_1=False):\n",
        "\n",
        "    # Note: the paper uses a different layer naming scheme.\n",
        "    # In this code, layer N corresponds to layer N+2 from the article.\n",
        "\n",
        "    with tf.compat.v1.variable_scope(\"generator\"):\n",
        "\n",
        "        # -----------------------------------------\n",
        "        # Space-to-depth layer\n",
        "\n",
        "        space2depth_l0 = tf.nn.space_to_depth(input, 2)                                         # 512 -> 256\n",
        "\n",
        "        # -----------------------------------------\n",
        "        # Downsampling layers\n",
        "\n",
        "        conv_l1_d1 = _conv_multi_block(space2depth_l0, 3, num_maps=32, instance_norm=False)     # 256 -> 256\n",
        "        pool1 = max_pool(conv_l1_d1, 2)                                                         # 256 -> 128\n",
        "\n",
        "        conv_l2_d1 = _conv_multi_block(pool1, 3, num_maps=64, instance_norm=instance_norm)      # 128 -> 128\n",
        "        pool2 = max_pool(conv_l2_d1, 2)                                                         # 128 -> 64\n",
        "\n",
        "        conv_l3_d1 = _conv_multi_block(pool2, 3, num_maps=128, instance_norm=instance_norm)     # 64 -> 64\n",
        "        pool3 = max_pool(conv_l3_d1, 2)                                                         # 64 -> 32\n",
        "\n",
        "        conv_l4_d1 = _conv_multi_block(pool3, 3, num_maps=256, instance_norm=instance_norm)     # 32 -> 32\n",
        "        pool4 = max_pool(conv_l4_d1, 2)                                                         # 32 -> 16\n",
        "\n",
        "        # -----------------------------------------\n",
        "        # Processing: Level 5,  Input size: 16 x 16\n",
        "\n",
        "        conv_l5_d1 = _conv_multi_block(pool4, 3, num_maps=512, instance_norm=instance_norm)\n",
        "        conv_l5_d2 = _conv_multi_block(conv_l5_d1, 3, num_maps=512, instance_norm=instance_norm) + conv_l5_d1\n",
        "        conv_l5_d3 = _conv_multi_block(conv_l5_d2, 3, num_maps=512, instance_norm=instance_norm) + conv_l5_d2\n",
        "        conv_l5_d4 = _conv_multi_block(conv_l5_d3, 3, num_maps=512, instance_norm=instance_norm)\n",
        "\n",
        "        conv_t4a = _conv_tranpose_layer(conv_l5_d4, 256, 3, 2)      # 16 -> 32\n",
        "        conv_t4b = _conv_tranpose_layer(conv_l5_d4, 256, 3, 2)      # 16 -> 32\n",
        "\n",
        "        # -> Output: Level 5\n",
        "\n",
        "        conv_l5_out = _conv_layer(conv_l5_d4, 3, 3, 1, relu=False, instance_norm=False)\n",
        "        output_l5 = tf.nn.tanh(conv_l5_out) * 0.58 + 0.5\n",
        "\n",
        "        # -----------------------------------------\n",
        "        # Processing: Level 4,  Input size: 32 x 32\n",
        "\n",
        "        conv_l4_d2 = stack(conv_l4_d1, conv_t4a)\n",
        "        conv_l4_d3 = _conv_multi_block(conv_l4_d2, 3, num_maps=256, instance_norm=instance_norm)\n",
        "        conv_l4_d4 = _conv_multi_block(conv_l4_d3, 3, num_maps=256, instance_norm=instance_norm) + conv_l4_d3\n",
        "        conv_l4_d5 = _conv_multi_block(conv_l4_d4, 3, num_maps=256, instance_norm=instance_norm) + conv_l4_d4\n",
        "        conv_l4_d6 = stack(_conv_multi_block(conv_l4_d5, 3, num_maps=256, instance_norm=instance_norm), conv_t4b)\n",
        "\n",
        "        conv_l4_d7 = _conv_multi_block(conv_l4_d6, 3, num_maps=256, instance_norm=instance_norm)\n",
        "\n",
        "        conv_t3a = _conv_tranpose_layer(conv_l4_d7, 128, 3, 2)      # 32 -> 64\n",
        "        conv_t3b = _conv_tranpose_layer(conv_l4_d7, 128, 3, 2)      # 32 -> 64\n",
        "\n",
        "        # -> Output: Level 4\n",
        "\n",
        "        conv_l4_out = _conv_layer(conv_l4_d7, 3, 3, 1, relu=False, instance_norm=False)\n",
        "        output_l4 = tf.nn.tanh(conv_l4_out) * 0.58 + 0.5\n",
        "\n",
        "        # -----------------------------------------\n",
        "        # Processing: Level 3,  Input size: 64 x 64\n",
        "\n",
        "        conv_l3_d2 = stack(conv_l3_d1, conv_t3a)\n",
        "        conv_l3_d3 = _conv_multi_block(conv_l3_d2, 5, num_maps=128, instance_norm=instance_norm) + conv_l3_d2\n",
        "        conv_l3_d4 = _conv_multi_block(conv_l3_d3, 5, num_maps=128, instance_norm=instance_norm) + conv_l3_d3\n",
        "        conv_l3_d5 = _conv_multi_block(conv_l3_d4, 5, num_maps=128, instance_norm=instance_norm) + conv_l3_d4\n",
        "        conv_l3_d6 = stack(_conv_multi_block(conv_l3_d5, 5, num_maps=128, instance_norm=instance_norm), conv_l3_d1)\n",
        "        conv_l3_d7 = stack(conv_l3_d6, conv_t3b)\n",
        "\n",
        "        conv_l3_d8 = _conv_multi_block(conv_l3_d7, 3, num_maps=128, instance_norm=instance_norm)\n",
        "\n",
        "        conv_t2a = _conv_tranpose_layer(conv_l3_d8, 64, 3, 2)       # 64 -> 128\n",
        "        conv_t2b = _conv_tranpose_layer(conv_l3_d8, 64, 3, 2)       # 64 -> 128\n",
        "\n",
        "        # -> Output: Level 3\n",
        "\n",
        "        conv_l3_out = _conv_layer(conv_l3_d8, 3, 3, 1, relu=False, instance_norm=False)\n",
        "        output_l3 = tf.nn.tanh(conv_l3_out) * 0.58 + 0.5\n",
        "\n",
        "        # -------------------------------------------\n",
        "        # Processing: Level 2,  Input size: 128 x 128\n",
        "\n",
        "        conv_l2_d2 = stack(conv_l2_d1, conv_t2a)\n",
        "        conv_l2_d3 = stack(_conv_multi_block(conv_l2_d2, 5, num_maps=64, instance_norm=instance_norm), conv_l2_d1)\n",
        "\n",
        "        conv_l2_d4 = _conv_multi_block(conv_l2_d3, 7, num_maps=64, instance_norm=instance_norm) + conv_l2_d3\n",
        "        conv_l2_d5 = _conv_multi_block(conv_l2_d4, 7, num_maps=64, instance_norm=instance_norm) + conv_l2_d4\n",
        "        conv_l2_d6 = _conv_multi_block(conv_l2_d5, 7, num_maps=64, instance_norm=instance_norm) + conv_l2_d5\n",
        "        conv_l2_d7 = stack(_conv_multi_block(conv_l2_d6, 7, num_maps=64, instance_norm=instance_norm), conv_l2_d1)\n",
        "\n",
        "        conv_l2_d8 = stack(_conv_multi_block(conv_l2_d7, 5, num_maps=64, instance_norm=instance_norm), conv_t2b)\n",
        "        conv_l2_d9 = _conv_multi_block(conv_l2_d8, 3, num_maps=64, instance_norm=instance_norm)\n",
        "\n",
        "        conv_t1a = _conv_tranpose_layer(conv_l2_d9, 32, 3, 2)       # 128 -> 256\n",
        "        conv_t1b = _conv_tranpose_layer(conv_l2_d9, 32, 3, 2)       # 128 -> 256\n",
        "\n",
        "        # -> Output: Level 2\n",
        "\n",
        "        conv_l2_out = _conv_layer(conv_l2_d9, 3, 3, 1, relu=False, instance_norm=False)\n",
        "        output_l2 = tf.nn.tanh(conv_l2_out) * 0.58 + 0.5\n",
        "\n",
        "        # -------------------------------------------\n",
        "        # Processing: Level 1,  Input size: 256 x 256\n",
        "\n",
        "        conv_l1_d2 = stack(conv_l1_d1, conv_t1a)\n",
        "        conv_l1_d3 = stack(_conv_multi_block(conv_l1_d2, 5, num_maps=32, instance_norm=False), conv_l1_d1)\n",
        "\n",
        "        conv_l1_d4 = _conv_multi_block(conv_l1_d3, 7, num_maps=32, instance_norm=False)\n",
        "\n",
        "        conv_l1_d5 = _conv_multi_block(conv_l1_d4, 9, num_maps=32, instance_norm=instance_norm_level_1)\n",
        "        conv_l1_d6 = _conv_multi_block(conv_l1_d5, 9, num_maps=32, instance_norm=instance_norm_level_1) + conv_l1_d5\n",
        "        conv_l1_d7 = _conv_multi_block(conv_l1_d6, 9, num_maps=32, instance_norm=instance_norm_level_1) + conv_l1_d6\n",
        "        conv_l1_d8 = _conv_multi_block(conv_l1_d7, 9, num_maps=32, instance_norm=instance_norm_level_1) + conv_l1_d7\n",
        "\n",
        "        conv_l1_d9 = stack(_conv_multi_block(conv_l1_d8, 7, num_maps=32, instance_norm=False), conv_l1_d1)\n",
        "\n",
        "        conv_l1_d10 = stack(_conv_multi_block(conv_l1_d9, 5, num_maps=32, instance_norm=False), conv_t1b)\n",
        "        conv_l1_d11 = stack(conv_l1_d10, conv_l1_d1)\n",
        "\n",
        "        conv_l1_d12 = _conv_multi_block(conv_l1_d11, 3, num_maps=32, instance_norm=False)\n",
        "\n",
        "        # -> Output: Level 1\n",
        "\n",
        "        conv_l1_out = _conv_layer(conv_l1_d12, 3, 3, 1, relu=False, instance_norm=False)\n",
        "        output_l1 = tf.nn.tanh(conv_l1_out) * 0.58 + 0.5\n",
        "\n",
        "        # ----------------------------------------------------------\n",
        "        # Processing: Level 0 (x2 upscaling),  Input size: 256 x 256\n",
        "\n",
        "        conv_l0 = _conv_tranpose_layer(conv_l1_d12, 8, 3, 2)        # 256 -> 512\n",
        "        conv_l0_out = _conv_layer(conv_l0, 3, 3, 1, relu=False, instance_norm=False)\n",
        "        output_l0 = tf.nn.tanh(conv_l0_out) * 0.58 + 0.5\n",
        "\n",
        "        # ----------------------------------------------------------\n",
        "        # Processing: Level Up (x4 upscaling),  Input size: 512 x 512\n",
        "\n",
        "        conv_l_up = _conv_tranpose_layer(conv_l0_out, 3, 3, 2)  # 512 -> 1024\n",
        "        conv_l_up_out = _conv_layer(conv_l_up, 3, 3, 1, relu=False, instance_norm=False)\n",
        "\n",
        "        output_l_up = tf.nn.tanh(conv_l_up_out) * 0.58 + 0.5\n",
        "\n",
        "    return output_l_up, output_l0, output_l1, output_l2, output_l3, output_l4, output_l5\n",
        "\n",
        "\n",
        "def _conv_multi_block(input, max_size, num_maps, instance_norm):\n",
        "\n",
        "    conv_3a = _conv_layer(input, num_maps, 3, 1, relu=True, instance_norm=instance_norm)\n",
        "    conv_3b = _conv_layer(conv_3a, num_maps, 3, 1, relu=True, instance_norm=instance_norm)\n",
        "\n",
        "    output_tensor = conv_3b\n",
        "\n",
        "    if max_size >= 5:\n",
        "\n",
        "        conv_5a = _conv_layer(input, num_maps, 5, 1, relu=True, instance_norm=instance_norm)\n",
        "        conv_5b = _conv_layer(conv_5a, num_maps, 5, 1, relu=True, instance_norm=instance_norm)\n",
        "\n",
        "        output_tensor = stack(output_tensor, conv_5b)\n",
        "\n",
        "    if max_size >= 7:\n",
        "\n",
        "        conv_7a = _conv_layer(input, num_maps, 7, 1, relu=True, instance_norm=instance_norm)\n",
        "        conv_7b = _conv_layer(conv_7a, num_maps, 7, 1, relu=True, instance_norm=instance_norm)\n",
        "\n",
        "        output_tensor = stack(output_tensor, conv_7b)\n",
        "\n",
        "    if max_size >= 9:\n",
        "\n",
        "        conv_9a = _conv_layer(input, num_maps, 9, 1, relu=True, instance_norm=instance_norm)\n",
        "        conv_9b = _conv_layer(conv_9a, num_maps, 9, 1, relu=True, instance_norm=instance_norm)\n",
        "\n",
        "        output_tensor = stack(output_tensor, conv_9b)\n",
        "\n",
        "    return output_tensor\n",
        "\n",
        "\n",
        "def stack(x, y):\n",
        "    return tf.concat([x, y], 3)\n",
        "\n",
        "\n",
        "def conv2d(x, W):\n",
        "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
        "\n",
        "\n",
        "def leaky_relu(x, alpha=0.2):\n",
        "    return tf.maximum(alpha * x, x)\n",
        "\n",
        "\n",
        "def _conv_layer(net, num_filters, filter_size, strides, relu=True, instance_norm=False, padding='SAME'):\n",
        "\n",
        "    weights_init = _conv_init_vars(net, num_filters, filter_size)\n",
        "    strides_shape = [1, strides, strides, 1]\n",
        "    bias = tf.Variable(tf.constant(0.01, shape=[num_filters]))\n",
        "\n",
        "    net = tf.nn.conv2d(net, weights_init, strides_shape, padding=padding) + bias\n",
        "\n",
        "    if instance_norm:\n",
        "        net = _instance_norm(net)\n",
        "\n",
        "    if relu:\n",
        "        net = leaky_relu(net)\n",
        "\n",
        "    return net\n",
        "\n",
        "\n",
        "def _instance_norm(net):\n",
        "\n",
        "    batch, rows, cols, channels = [i.value for i in net.get_shape()]\n",
        "    var_shape = [channels]\n",
        "\n",
        "    mu, sigma_sq = tf.compat.v1.nn.moments(net, [1,2], keep_dims=True)\n",
        "    shift = tf.Variable(tf.zeros(var_shape))\n",
        "    scale = tf.Variable(tf.ones(var_shape))\n",
        "\n",
        "    epsilon = 1e-3\n",
        "    normalized = (net-mu)/(sigma_sq + epsilon)**(.5)\n",
        "\n",
        "    return scale * normalized + shift\n",
        "\n",
        "\n",
        "def _conv_init_vars(net, out_channels, filter_size, transpose=False):\n",
        "\n",
        "    _, rows, cols, in_channels = [i.value for i in net.get_shape()]\n",
        "\n",
        "    if not transpose:\n",
        "        weights_shape = [filter_size, filter_size, in_channels, out_channels]\n",
        "    else:\n",
        "        weights_shape = [filter_size, filter_size, out_channels, in_channels]\n",
        "\n",
        "    weights_init = tf.Variable(tf.compat.v1.truncated_normal(weights_shape, stddev=0.01, seed=1), dtype=tf.float32)\n",
        "    return weights_init\n",
        "\n",
        "\n",
        "def _conv_tranpose_layer(net, num_filters, filter_size, strides):\n",
        "    weights_init = _conv_init_vars(net, num_filters, filter_size, transpose=True)\n",
        "\n",
        "    net_shape = tf.shape(net)\n",
        "    tf_shape = tf.stack([net_shape[0], net_shape[1] * strides, net_shape[2] * strides, num_filters])\n",
        "\n",
        "    strides_shape = [1, strides, strides, 1]\n",
        "    net = tf.nn.conv2d_transpose(net, weights_init, tf_shape, strides_shape, padding='SAME')\n",
        "\n",
        "    return leaky_relu(net)\n",
        "\n",
        "\n",
        "def max_pool(x, n):\n",
        "    return tf.nn.max_pool(x, ksize=[1, n, n, 1], strides=[1, n, n, 1], padding='VALID')\n",
        "#********************************MODEL.py************************************************************\n",
        "\n",
        "\n",
        "\n",
        "#********************************train_model.py************************************************************\n",
        "\n",
        "tf.compat.v1.disable_v2_behavior()\n",
        "\n",
        "# Processing command arguments\n",
        "\n",
        "# LEVEL, batch_size, train_size, learning_rate, restore_iter, num_train_iters, dataset_dir, vgg_dir, eval_step = \\\n",
        "    # process_command_args(sys.argv)\n",
        "\n",
        "for LEVEL in range(7,1,-1):\n",
        "  batch_size = 20\n",
        "  train_size = 2000\n",
        "  learning_rate = 5e-5\n",
        "  restore_iter = None\n",
        "  num_train_iters = 5000\n",
        "  dataset_dir = \"ebb_dataset/\"\n",
        "  vgg_dir = \"vgg_pretrained/imagenet-vgg-verydeep-19.mat\"\n",
        "  eval_step = 1000\n",
        "\n",
        "  # Defining the size of the input and target image patches\n",
        "\n",
        "  PATCH_WIDTH, PATCH_HEIGHT = 512, 512\n",
        "  DSLR_SCALE = float(1) / (2 ** (LEVEL - 2))\n",
        "\n",
        "  TARGET_WIDTH = int(PATCH_WIDTH * DSLR_SCALE)\n",
        "  TARGET_HEIGHT = int(PATCH_HEIGHT * DSLR_SCALE)\n",
        "  TARGET_DEPTH = 3\n",
        "  TARGET_SIZE = TARGET_WIDTH * TARGET_HEIGHT * TARGET_DEPTH\n",
        "\n",
        "  np.random.seed(0)\n",
        "\n",
        "  # Defining the model architecture\n",
        "\n",
        "  with tf.Graph().as_default(), tf.compat.v1.Session() as sess:\n",
        "      \n",
        "      # Placeholders for training data\n",
        "\n",
        "      input_ = tf.compat.v1.placeholder(tf.float32, [batch_size, PATCH_HEIGHT, PATCH_WIDTH, 4])\n",
        "      target_ = tf.compat.v1.placeholder(tf.float32, [batch_size, TARGET_HEIGHT, TARGET_WIDTH, TARGET_DEPTH])\n",
        "\n",
        "      # Get the rendered bokeh image\n",
        "\n",
        "      output_l1, output_l2, output_l3, output_l4, output_l5, output_l6, output_l7 = \\\n",
        "          PyNET(input_, instance_norm=True, instance_norm_level_1=False)\n",
        "\n",
        "      if LEVEL == 7:\n",
        "          bokeh_img = output_l7\n",
        "      if LEVEL == 6:\n",
        "          bokeh_img = output_l6\n",
        "      if LEVEL == 5:\n",
        "          bokeh_img = output_l5\n",
        "      if LEVEL == 4:\n",
        "          bokeh_img = output_l4\n",
        "      if LEVEL == 3:\n",
        "          bokeh_img = output_l3\n",
        "      if LEVEL == 2:\n",
        "          bokeh_img = output_l2\n",
        "      if LEVEL == 1:\n",
        "          bokeh_img = output_l1\n",
        "\n",
        "      # Losses\n",
        "\n",
        "      bokeh_img_flat = tf.reshape(bokeh_img, [-1, TARGET_SIZE])\n",
        "      target_flat = tf.reshape(target_, [-1, TARGET_SIZE])\n",
        "\n",
        "      # MSE loss\n",
        "      loss_mse = tf.reduce_sum(tf.pow(target_flat - bokeh_img_flat, 2)) / (TARGET_SIZE * batch_size)\n",
        "\n",
        "      # PSNR loss\n",
        "      loss_psnr = 20 * log10(1.0 / tf.sqrt(loss_mse))\n",
        "\n",
        "      # SSIM loss\n",
        "      loss_ssim = tf.reduce_mean(tf.image.ssim(bokeh_img, target_, 1.0))\n",
        "\n",
        "      # MS-SSIM loss\n",
        "      loss_ms_ssim = tf.reduce_mean(tf.image.ssim_multiscale(bokeh_img, target_, 1.0))\n",
        "\n",
        "      # L1 loss\n",
        "      loss_l1 = tf.compat.v1.losses.absolute_difference(bokeh_img, target_)\n",
        "\n",
        "      # Content loss\n",
        "      CONTENT_LAYER = 'relu5_4'\n",
        "\n",
        "      bokeh_img_vgg = net(vgg_dir, preprocess(bokeh_img * 255))\n",
        "      target_vgg = net(vgg_dir, preprocess(target_ * 255))\n",
        "\n",
        "      content_size = _tensor_size(target_vgg[CONTENT_LAYER]) * batch_size\n",
        "      loss_content = 2 * tf.nn.l2_loss(bokeh_img_vgg[CONTENT_LAYER] - target_vgg[CONTENT_LAYER]) / content_size\n",
        "\n",
        "      # Final loss function\n",
        "\n",
        "      if LEVEL > 1:\n",
        "          loss_generator = loss_l1 * 100\n",
        "      else:\n",
        "          loss_generator = loss_l1 * 10 + loss_content * 0.1 + (1 - loss_ssim) * 10\n",
        "\n",
        "      # Optimize network parameters\n",
        "\n",
        "      generator_vars = [v for v in tf.compat.v1.global_variables() if v.name.startswith(\"generator\")]\n",
        "      train_step_gen = tf.compat.v1.train.AdamOptimizer(learning_rate).minimize(loss_generator)\n",
        "\n",
        "      # Initialize and restore the variables\n",
        "\n",
        "      print(\"Initializing variables\")\n",
        "      sess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "      saver = tf.compat.v1.train.Saver(var_list=generator_vars, max_to_keep=100)\n",
        "\n",
        "      if LEVEL < 7:\n",
        "          print(\"Restoring Variables\")\n",
        "          saver.restore(sess, \"models/pynet_level_\" + str(LEVEL + 1) + \"_iteration_\" + str(restore_iter) + \".ckpt\")\n",
        "\n",
        "      saver = tf.compat.v1.train.Saver(var_list=generator_vars, max_to_keep=100)\n",
        "\n",
        "      # Loading training and test data\n",
        "\n",
        "      print(\"Loading test data...\")\n",
        "      test_data, test_answ = load_test_data(dataset_dir, PATCH_WIDTH, PATCH_HEIGHT, DSLR_SCALE)\n",
        "      print(\"Test data was loaded\\n\")\n",
        "\n",
        "      print(\"Loading training data...\")\n",
        "      train_data, train_answ = load_training_batch(dataset_dir, PATCH_WIDTH, PATCH_HEIGHT, DSLR_SCALE, train_size)\n",
        "      print(\"Training data was loaded\\n\")\n",
        "\n",
        "      TEST_SIZE = test_data.shape[0]\n",
        "      num_test_batches = int(test_data.shape[0] / batch_size)\n",
        "\n",
        "      visual_crops_ids = np.random.randint(0, TEST_SIZE, batch_size)\n",
        "      visual_test_crops = test_data[visual_crops_ids, :]\n",
        "      visual_target_crops = test_answ[visual_crops_ids, :]\n",
        "\n",
        "      print(\"Training network\")\n",
        "\n",
        "      logs = open(\"models/logs.txt\", \"w+\")\n",
        "      logs.close()\n",
        "\n",
        "      training_loss = 0.0\n",
        "\n",
        "      for i in range(num_train_iters + 1):\n",
        "\n",
        "          # Train PyNET model\n",
        "\n",
        "          idx_train = np.random.randint(0, train_size, batch_size)\n",
        "\n",
        "          phone_images = train_data[idx_train]\n",
        "          dslr_images = train_answ[idx_train]\n",
        "\n",
        "          # Random flips and rotations\n",
        "\n",
        "          for k in range(batch_size):\n",
        "\n",
        "              random_rotate = np.random.randint(1, 100) % 4\n",
        "              phone_images[k] = np.rot90(phone_images[k], random_rotate)\n",
        "              dslr_images[k] = np.rot90(dslr_images[k], random_rotate)\n",
        "              random_flip = np.random.randint(1, 100) % 2\n",
        "\n",
        "              if random_flip == 1:\n",
        "                  phone_images[k] = np.flipud(phone_images[k])\n",
        "                  dslr_images[k] = np.flipud(dslr_images[k])\n",
        "\n",
        "          # Training step\n",
        "\n",
        "          [loss_temp, temp] = sess.run([loss_generator, train_step_gen], feed_dict={input_: phone_images, target_: dslr_images})\n",
        "          training_loss += loss_temp / eval_step\n",
        "\n",
        "          if i % eval_step == 0:\n",
        "\n",
        "              # Evaluate PyNET model\n",
        "\n",
        "              test_losses = np.zeros((1, 6 if LEVEL < 4 else 5))\n",
        "\n",
        "              for j in range(num_test_batches):\n",
        "\n",
        "                  be = j * batch_size\n",
        "                  en = (j+1) * batch_size\n",
        "\n",
        "                  phone_images = test_data[be:en]\n",
        "                  dslr_images = test_answ[be:en]\n",
        "\n",
        "                  if LEVEL < 4:\n",
        "                      losses = sess.run([loss_generator, loss_content, loss_mse, loss_psnr, loss_l1, loss_ms_ssim], \\\n",
        "                                      feed_dict={input_: phone_images, target_: dslr_images})\n",
        "                  else:\n",
        "                      losses = sess.run([loss_generator, loss_content, loss_mse, loss_psnr, loss_l1], \\\n",
        "                                        feed_dict={input_: phone_images, target_: dslr_images})\n",
        "\n",
        "                  test_losses += np.asarray(losses) / num_test_batches\n",
        "\n",
        "              if LEVEL < 4:\n",
        "                  logs_gen = \"step %d | training: %.4g, test: %.4g | content: %.4g, mse: %.4g, psnr: %.4g, l1: %.4g, \" \\\n",
        "                            \"ms-ssim: %.4g\\n\" % (i, training_loss, test_losses[0][0], test_losses[0][1],\n",
        "                                                  test_losses[0][2], test_losses[0][3], test_losses[0][4], test_losses[0][5])\n",
        "              else:\n",
        "                  logs_gen = \"step %d | training: %.4g, test: %.4g | content: %.4g, mse: %.4g, psnr: %.4g, l1: %.4g\\n\" % \\\n",
        "                        (i, training_loss, test_losses[0][0], test_losses[0][1], test_losses[0][2], test_losses[0][3], test_losses[0][4])\n",
        "              print(logs_gen)\n",
        "\n",
        "              # Save the results to log file\n",
        "\n",
        "              logs = open(\"models/logs.txt\", \"a\")\n",
        "              logs.write(logs_gen)\n",
        "              logs.write('\\n')\n",
        "              logs.close()\n",
        "\n",
        "              # Save visual results for several test images\n",
        "\n",
        "              bokeh_crops = sess.run(bokeh_img, feed_dict={input_: visual_test_crops, target_: dslr_images})\n",
        "\n",
        "              idx = 0\n",
        "              for crop in bokeh_crops:\n",
        "                  if idx < 7:\n",
        "                      before_after = np.hstack((\n",
        "                                      np.float32(imageio.imresize(\n",
        "                                          np.reshape(visual_test_crops[idx, :, :, 0:3] * 255, [PATCH_HEIGHT, PATCH_WIDTH, 3]),\n",
        "                                                    [TARGET_HEIGHT, TARGET_WIDTH])) / 255.0,\n",
        "                                      crop,\n",
        "                                      np.reshape(visual_target_crops[idx], [TARGET_HEIGHT, TARGET_WIDTH, TARGET_DEPTH])))\n",
        "                      imageio.imsave(\"results/pynet_img_\" + str(idx) + \"_level_\" + str(LEVEL) + \"_iter_\" + str(i) + \".jpg\",\n",
        "                                  before_after)\n",
        "                  idx += 1\n",
        "\n",
        "              training_loss = 0.0\n",
        "\n",
        "              # Saving the model that corresponds to the current iteration\n",
        "              saver.save(sess, \"models/pynet_level_\" + str(LEVEL) + \"_iteration_\" + str(i) + \".ckpt\", write_meta_graph=False)\n",
        "\n",
        "          # Loading new training data\n",
        "          if i % 1000 == 0:\n",
        "\n",
        "              del train_data\n",
        "              del train_answ\n",
        "              train_data, train_answ = load_training_batch(dataset_dir, PATCH_WIDTH, PATCH_HEIGHT, DSLR_SCALE, train_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 971
        },
        "id": "eR6psfFrIp6W",
        "outputId": "aaea5c17-4743-4875-956b-b582070a15e8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "no errors here\n",
            "no errors here\n",
            "no errors here\n",
            "no errors here\n",
            "no errors here\n",
            "no errors here\n",
            "no errors here\n",
            "no errors here\n",
            "no errors here\n",
            "no errors here\n",
            "no errors here\n",
            "no errors here\n",
            "no errors here\n",
            "no errors here\n",
            "no errors here\n",
            "no errors here\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-97d5df40bb64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m       \u001b[0mbokeh_img_vgg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvgg_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbokeh_img\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m       \u001b[0mtarget_vgg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvgg_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m       \u001b[0mcontent_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_vgg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONTENT_LAYER\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-97d5df40bb64>\u001b[0m in \u001b[0;36mnet\u001b[0;34m(path_to_vgg_net, input_image)\u001b[0m\n\u001b[1;32m     39\u001b[0m     )\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_vgg_net\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'layers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0mMR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0mmatfile_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmdict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/io/matlab/mio5.py\u001b[0m in \u001b[0;36mget_variables\u001b[0;34m(self, variable_names)\u001b[0m\n\u001b[1;32m    330\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_var_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhdr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mMatReadError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                 warnings.warn(\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/scipy/io/matlab/mio5.py\u001b[0m in \u001b[0;36mread_var_array\u001b[0;34m(self, header, process)\u001b[0m\n\u001b[1;32m    290\u001b[0m            \u001b[0;31m`\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         '''\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_matrix_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_from_header\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mmio5_utils.pyx\u001b[0m in \u001b[0;36mscipy.io.matlab.mio5_utils.VarReader5.array_from_header\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mmio5_utils.pyx\u001b[0m in \u001b[0;36mscipy.io.matlab.mio5_utils.VarReader5.array_from_header\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mmio5_utils.pyx\u001b[0m in \u001b[0;36mscipy.io.matlab.mio5_utils.VarReader5.read_cells\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mmio5_utils.pyx\u001b[0m in \u001b[0;36mscipy.io.matlab.mio5_utils.VarReader5.read_mi_matrix\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mmio5_utils.pyx\u001b[0m in \u001b[0;36mscipy.io.matlab.mio5_utils.VarReader5.array_from_header\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mmio5_utils.pyx\u001b[0m in \u001b[0;36mscipy.io.matlab.mio5_utils.VarReader5.read_struct\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mmio5_utils.pyx\u001b[0m in \u001b[0;36mscipy.io.matlab.mio5_utils.VarReader5.read_mi_matrix\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mmio5_utils.pyx\u001b[0m in \u001b[0;36mscipy.io.matlab.mio5_utils.VarReader5.array_from_header\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mmio5_utils.pyx\u001b[0m in \u001b[0;36mscipy.io.matlab.mio5_utils.VarReader5.read_char\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}